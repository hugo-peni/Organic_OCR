{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyOsNLSfExOOgqwVXVbfX3Gs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6h_AhmuA4GUq","executionInfo":{"status":"ok","timestamp":1747719115773,"user_tz":420,"elapsed":14584,"user":{"displayName":"hugo penichou","userId":"06336513948523130146"}},"outputId":"a86da6b1-ec45-4c93-dc2e-95400e9f8dae"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import cv2\n","import cv2.aruco as aruco\n","from google.colab.patches import cv2_imshow # Import cv2_imshow\n","\n","# Load your image (replace with your image path or use a frame from a video)\n","image = cv2.imread(\"/content/drive/My Drive/pagetrack/Detect_ArUco_marker.png\")\n","\n","# Convert image to grayscale (required for detection)\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","# Define the dictionary of markers you used\n","aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n","\n","# Initialize the detector parameters using default values\n","parameters = aruco.DetectorParameters()\n","\n","# Create the detector\n","detector = aruco.ArucoDetector(aruco_dict, parameters)\n","\n","# Detect the markers in the image\n","corners, ids, rejected = detector.detectMarkers(gray)\n","\n","# Draw detected markers on the image\n","if ids is not None:\n","    print(\"Detected marker IDs:\", ids.flatten())\n","    aruco.drawDetectedMarkers(image, corners, ids)\n","else:\n","    print(\"No markers detected.\")\n","\n","# Show the result using cv2_imshow\n","cv2_imshow(image)\n","\n","# Remove cv2.waitKey(0) and cv2.destroyAllWindows() as they are not needed with cv2_imshow\n","# cv2.waitKey(0)\n","# cv2.destroyAllWindows()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":766,"output_embedded_package_id":"1ZSnGFhvC5wkWr5A8ZG_VKt1jtakTV3AN"},"id":"nKPNglrs4dl-","executionInfo":{"status":"ok","timestamp":1747719119538,"user_tz":420,"elapsed":2378,"user":{"displayName":"hugo penichou","userId":"06336513948523130146"}},"outputId":"2e7bdac9-0440-4014-a24a-7b7c72d20f09"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["!pip install transformers datasets torch torchvision\n","!pip install opencv-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_bcsULPSQPf","executionInfo":{"status":"ok","timestamp":1747720956233,"user_tz":420,"elapsed":4823,"user":{"displayName":"hugo penichou","userId":"06336513948523130146"}},"outputId":"e176eae5-223e-4b66-bc0c-89fedc22c3d7"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"]}]},{"cell_type":"code","source":["from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n","from PIL import Image\n","import torch\n","\n","# Load model and processor\n","processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n","model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n","\n","# Load image\n","image_path = \"/content/drive/My Drive/pagetrack/OCR_test.jpg\"\n","image = Image.open(image_path).convert(\"RGB\")\n","\n","# Prepare image\n","pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n","\n","# Inference\n","generated_ids = model.generate(pixel_values)\n","generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","# Save to file\n","with open(\"/content/drive/My Drive/pagetrack/handwriting_output.txt\", \"w\") as f:\n","    f.write(generated_text)\n","\n","print(\"✅ OCR done. Output saved to handwriting_output.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ve1anXh3SXqP","executionInfo":{"status":"ok","timestamp":1747721008869,"user_tz":420,"elapsed":1877,"user":{"displayName":"hugo penichou","userId":"06336513948523130146"}},"outputId":"b24fb22b-80e2-4ddf-8402-3789ff0db983"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"image_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"pooler_act\": \"tanh\",\n","  \"pooler_output_size\": 768,\n","  \"qkv_bias\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.51.3\"\n","}\n","\n","Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_cross_attention\": true,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": 0.0,\n","  \"cross_attention_hidden_size\": 768,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"eos_token_id\": 2,\n","  \"init_std\": 0.02,\n","  \"is_decoder\": true,\n","  \"layernorm_embedding\": true,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"trocr\",\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": false,\n","  \"use_learned_position_embeddings\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ OCR done. Output saved to handwriting_output.txt\n"]}]},{"cell_type":"code","source":["from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n","from PIL import Image\n","import torch\n","\n","# Load model & processor\n","processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n","model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n","model.eval()  # <-- important for consistent results\n","\n","# Load & preprocess image\n","image = Image.open(\"/content/drive/My Drive/pagetrack/OCR_test.jpg\").convert(\"RGB\")\n","\n","# Optional: upscale if handwriting is small\n","# image = image.resize((image.width * 2, image.height * 2))\n","\n","# Prepare inputs\n","pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n","\n","# Generate output\n","with torch.no_grad():  # important for inference\n","    generated_ids = model.generate(pixel_values)\n","\n","# Decode output\n","generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","# Save to file\n","with open(\"/content/drive/My Drive/pagetrack/handwriting_output.txt\", \"w\") as f:\n","    f.write(generated_text)\n","\n","print(\"✅ Result:\", generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBrzCfWoS1f5","executionInfo":{"status":"ok","timestamp":1747721118711,"user_tz":420,"elapsed":5606,"user":{"displayName":"hugo penichou","userId":"06336513948523130146"}},"outputId":"fae0d3f0-1970-4e33-8a3e-7c852e5f35e2"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"image_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"pooler_act\": \"tanh\",\n","  \"pooler_output_size\": 768,\n","  \"qkv_bias\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.51.3\"\n","}\n","\n","Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_cross_attention\": true,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": 0.0,\n","  \"cross_attention_hidden_size\": 768,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"eos_token_id\": 2,\n","  \"init_std\": 0.02,\n","  \"is_decoder\": true,\n","  \"layernorm_embedding\": true,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"trocr\",\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": false,\n","  \"use_learned_position_embeddings\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Result: 0 0\n"]}]},{"cell_type":"code","source":["image.show()  # Make sure the image is readable, not cropped or rotated"],"metadata":{"id":"nNsyFOdZTBPK","executionInfo":{"status":"ok","timestamp":1747721146329,"user_tz":420,"elapsed":1490,"user":{"displayName":"hugo penichou","userId":"06336513948523130146"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["import cv2\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Load image\n","img_path = \"/content/drive/My Drive/pagetrack/OCR_test.jpg\"  # Upload your image to Colab first\n","image = cv2.imread(img_path)\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","# Optional: improve contrast\n","gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n","\n","# Save preprocessed image\n","cv2.imwrite(\"/content/drive/My Drive/pagetrack/processed.png\", gray)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2e_G3xq5LkTe","executionInfo":{"status":"ok","timestamp":1747719514414,"user_tz":420,"elapsed":137,"user":{"displayName":"hugo penichou","userId":"06336513948523130146"}},"outputId":"795ef6d9-fffb-440f-981f-9d553cfcf57f"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]}]}